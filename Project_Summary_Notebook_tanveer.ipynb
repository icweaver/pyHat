{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank-normalization, folding, and localization: An improved $\\widehat{R}$ for assessing convergence of MCMC by Vehtari et al. 2019\n",
    "\n",
    "### Students: Tanveer Karim and Ian Weaver\n",
    "### Course: APMTH 207\n",
    "### GitHub Repository: https://github.com/icweaver/pyhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement and Existing Work\n",
    "The traditional $\\widehat{R}$ statistic was first defined by Gelman and Rubin in 1992 and the subsequent revision involving the split variation was defined by Gelman et al. 2013. These statistics theoretically measure the variance between chains and in chains for an MCMC model and quantifies whether the chains have not mixed well, i.e. whether they have diverged. Along with traceplots, these statistics serve as a powerful way to diagnose whether the MCMC samples can be used for subsequent analysis. \n",
    "\n",
    "However, Vehtari et al. 2019 showed that the well-known $\\widehat{R}$ statistic and traceplots have limitations and fail in many cases. Thus, they propose an alternative definition of $\\widehat{R}$ and introduce rankplots as an alternative to traceplots as visual diagnostics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context and Unique Contribution\n",
    "\n",
    "For anyone doing Bayesian analysis over multidimensions, MCMC samplers are paramount for modelling purposes. Beyond 2D, visualizing convergence of MCMC samplers on the parameter space becomes very complicated and we have to resort to alternate methods to understand convergence. Hence, if our well-used statistic and diagnostics are not performing as we expect them to, then almost any complex multidimensional MCMC modelling becomes suspect. Because of this importance, we decided to tackle this paper and understand how to best study convergence issues related to MCMC samplers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Content\n",
    "\n",
    "The key problems that Vehtari et al. 2019 identified with the existing $\\widehat{R}$ is that it was only sensitive to errors in the first moment and it failed catastrophically when the variance was infinite. Their proposed $\\widehat{R}$ utilizes the concept of rank normalization. \n",
    "\n",
    "Rank normalization essentially converts the chains from values to ranks for a given parameter and then converts the ranks to quantiles of a standard normal. While this procedure destroys any information about the values within the chains themselves, it nevertheless converts the chains to nice standard normals which are easy to operate with. Afterwards, we can run the traditional $\\widehat{R}$ formula on the rank normalized values to get the ranked $\\widehat{R}$. \n",
    "\n",
    "This conversion makes the rank normalized $\\widehat{R}$ robust against heavy tails and makes it sensitive to changes in tails, especially for the folded ranked $\\widehat{R}$ statistic, which is sensitive to scale. Moreover, rank normalization makes $\\widehat{R}$ parameterization invariant. \n",
    "\n",
    "In addition, the rankplots are generated by producing histograms of ranks of $n$ chains. If the chains sample the same underlying distribution, then the rankplots will look similar because they would randomly sample the space the same way after burn-in and thinning. Any deviation from this behaviour would imply that different chains are exploring different parts of the parameter space and have not mixed in well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "In ``examples/gaussian/toymodel_gaussian.ipynb``, we show a proof-of-concept of the new $\\widehat{R}$ statistic along with rankplot to see whether Vehtari et al. 2019's proposals make sense. We sampled a standard normal with $4$ chains and replaced $1$ of the chains with a chain of a different model that was sampled from a different normal distribution (different variance). Our hypothesis was that since both the regular chains and the fake chain share the same mean, the traditional $\\widehat{R}$ would not pick up the difference in variance but the ranked $\\widehat{R}$ would. It turned out that indeed our speculation was correct as the folded rank $\\widehat{R}$ statistic is robust against changes in tails and picked up this fake chain easily. Moreover, the rankplots clearly showed which chain was having issues.\n",
    "\n",
    "While the previous experiment was sound and confirmed the paper, it was only in $1$D and we wanted to scale it to see whether it worked for a $2$D Gaussian. Furthermore, we wanted to explore the question as to whether these diagnostics would be able to understand the difference between two Gaussians with the same mean but different correlation between the features -- in our case one was isotropic and the other case was heavily correlated. We showcase this experiment in ``examples/gaussian/toymodel_2DGaussian.ipynb``.\n",
    "\n",
    "In this second experiment, we found that the rank $\\widehat{R}$ and rankplots are not robust at picking up difference between isotropy and correlation. We injected a fake correlated chain into a model of an isotropic Gaussian. The metrics proposed in the paper were not able to pick up the difference. On the other hand, when we injected a fake correlated chain that also had a difference in variance, i.e. different diagonal terms compared to the isotropic Gaussian, then these diagnostic methods picked up the difference easily. This suggested that rank $\\widehat{R}$ and rankplots are sensitive to diagonal terms in covariance matrices, but are not sensitive at all to off-diagonal terms.\n",
    "\n",
    "**Ian: exoplanet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "When we compare the results between the 1D and the 2D Gaussian cases, we see that the ranked $\\widehat{R}$ and rankplots are sensitive to changes in variance terms but not in covariance terms. This makes sense because the way $\\widehat{R}$ is defined, it uses marginalized information for a specific parameter. So for evaluating whether marginalized chains have mixed well or not, ranked $\\widehat{R}$ and rankplots are quite powerful and prove the point of the paper.\n",
    "\n",
    "However, we found that one should exercise caution when using rank $\\widehat{R}$ and rankplots at claiming that they indicate chains are sampling from the same underlying distribution. We clearly showed that even when chains explore different underlying distributions, the diagnostics could give values that are consistent with chains exploring the same distribution. **This implies that ranked $\\widehat{R}$ and rankplots can only tell us whether chains have NOT mixed well**; it does not tell us anything about whether chains HAVE mixed well.\n",
    "\n",
    "**Ian: exoplanet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "For future work, we would like to understand how to modify $\\widehat{R}$ in a way such that it captures correlation terms from the covariance matrix. Right now, off-diagonal terms do not contribute at all, as shown in ``examples/gausssian/toymodel_2dGaussian.ipynb``. We would like to explore and understand if there is a better way quantify changes in the correlation terms. One possible avenue is to understand the relationship between $\\widehat{R}$ and autocorrelation function and see if there is a way to unify them together. \n",
    "\n",
    "**Ian: exoplanet**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
